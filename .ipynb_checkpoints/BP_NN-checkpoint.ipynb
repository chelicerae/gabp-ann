{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width  class\n",
       "0           6.8          3.0           5.5          2.1      2\n",
       "1           6.3          2.9           5.6          1.8      2\n",
       "2           5.7          2.8           4.5          1.3      1\n",
       "3           5.6          2.8           4.9          2.0      2\n",
       "4           6.2          2.8           4.8          1.8      2\n",
       "5           5.8          2.6           4.0          1.2      1\n",
       "6           6.5          3.0           5.8          2.2      2\n",
       "7           7.7          3.0           6.1          2.3      2\n",
       "8           6.3          3.3           4.7          1.6      1\n",
       "9           5.4          3.7           1.5          0.2      0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('iris.data', header=0)\n",
    "dataset['class'] = dataset['class'].map({'Iris-setosa':0, \n",
    "                                         'Iris-versicolor':1, \n",
    "                                         'Iris-virginica':2}).astype(int)\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[:100]\n",
    "test = dataset[100:]\n",
    "\n",
    "X = train.as_matrix(['sepal length', \n",
    "                   'sepal width', \n",
    "                   'petal length', \n",
    "                   'petal width'])\n",
    "y = train.as_matrix(['class'])\n",
    "y = to_categorical(y)\n",
    "\n",
    "X_test = test.as_matrix(['sepal length', \n",
    "                   'sepal width', \n",
    "                   'petal length', \n",
    "                   'petal width'])\n",
    "y_test = test.as_matrix(['class'])\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No bias results 50 000 iterations (-2, 2)\n",
    "\n",
    "# all_weights = np.array([ 0.82850905,  0.40072448, -1.16062222,  0.24129967, -0.00901011,\n",
    "#        -0.55042084,  0.17620743, -0.35640638,  0.28189825, -0.40337483,\n",
    "#        -0.53330681,  0.1720898 ,  1.99998587, -0.0237903 , -1.99372395,\n",
    "#        -1.53997806,  1.31316753, -1.85079916,  1.53083946,  1.20376128,\n",
    "#         2.        , -1.03966506, -1.78537761, -0.57303334, -0.83020559,\n",
    "#         1.2181045 , -0.95560658,  0.32555322, -0.01306465, -0.58740851,\n",
    "#        -0.30109983,  2.        ,  1.7863003 ,  1.87352263, -1.2398616 ,\n",
    "#        -1.35349408,  0.21784511,  2.        , -1.14411966, -0.96291608,\n",
    "#         1.15340453, -0.12331424, -0.68801667,  1.76125887, -1.96991071,\n",
    "#         0.88347918,  0.65414193, -0.13644339,  1.44238258,  1.31562074,\n",
    "#        -0.95010176, -0.08231172, -0.42752867,  1.66501362,  0.62532375,\n",
    "#         1.99714638,  0.73385111, -1.89800158, -0.01269442,  2.        ,\n",
    "#        -0.34806914, -1.06161126,  0.13609927, -1.90406267,  1.64843287,\n",
    "#        -1.89691344,  1.50997576,  1.82210462, -1.48688357, -2.        ])\n",
    "\n",
    "# Bias results 1000 iterations (-100, 100)\n",
    "\n",
    "# all_weights = np.array([-55.99912628,  -2.94074698,  48.62230211,  27.70564858,\n",
    "#        -14.72015624,  -9.42324088, -43.03292206, -18.40727773,\n",
    "#        -43.64839552, -58.04569456, -50.20950896, -85.52187718,\n",
    "#         70.56984522,   5.20257721,  -0.26611847,  83.19208242,\n",
    "#        -72.17945228,  49.45460294,  -2.2190893 ,   4.7459282 ,\n",
    "#        -83.69299758,  58.1335316 , -15.73228806, -34.88828344,\n",
    "#         78.57904655,  68.61037977,  13.15403183,  89.08639842,\n",
    "#         36.90008999, -43.38496429,  93.1164013 ,  10.73194315,\n",
    "#         93.14897671, -50.62290616,   8.02729018,  96.21904633,\n",
    "#         17.4439362 ,  25.44069502,  68.22808982,  -7.54487909,\n",
    "#        -99.80341536, -64.52404744,  42.77614549, -76.405061  ,\n",
    "#        -72.08232071,  14.85601168, -53.68871622,  36.09791257,\n",
    "#        -22.96207116,  71.75095558,  84.62201604, -66.19849963,\n",
    "#        -89.07164706,  65.08517806, -42.65904316,  61.44426617,\n",
    "#         28.48123545, -11.79782272,  33.36877307,  73.80732532,\n",
    "#        -97.63618378,  63.64767343, -59.50050103, -82.1135804 ,\n",
    "#         77.1921181 , -49.07060001, -83.14296263,  20.58923936,\n",
    "#         22.45407113, -60.34021219,  93.07392893,  66.8388894 ,\n",
    "#        -53.42301689, -64.53013567,  34.68704473,  40.19995135,\n",
    "#        -20.92426576,  -6.45971336, -27.03420895,  -5.00038901,\n",
    "#        -65.3913821 , -31.87113074,  43.80992814])\n",
    "\n",
    "# Bias results 1000 iterations (-2, 2)\n",
    "\n",
    "# all_weights = np.array([ 0.44539658, -0.54400259,  0.4147466 ,  1.1843882 ,  1.55279398,\n",
    "#         0.46813689, -1.32291181, -1.99987314, -0.49823897,  1.3223775 ,\n",
    "#         0.51995836,  0.86037499, -1.60616985, -1.24742032,  0.12282469,\n",
    "#         0.79423939,  0.95008809,  1.69781401, -1.91294032, -1.82596235,\n",
    "#        -0.2203242 , -1.23333196, -0.11080954, -0.69195492,  0.16182995,\n",
    "#        -0.70553753,  1.87664087,  0.23026903,  0.7471049 , -1.7373532 ,\n",
    "#         1.36182834, -0.5408667 , -0.94710073, -0.23820658, -0.69248999,\n",
    "#         0.57001043, -0.33089435,  0.22820795,  1.8313249 , -0.35236008,\n",
    "#         0.30297719,  1.93726387,  0.92641075, -0.62700894,  1.82442043,\n",
    "#        -1.01640387, -1.99864327, -0.06462852, -1.53881716,  0.48566329,\n",
    "#        -1.64920395,  1.99999965,  1.75121857,  0.52660668,  1.21734099,\n",
    "#        -0.45846083, -1.32921847, -1.4606413 ,  0.93423697, -0.86710597,\n",
    "#        -1.13187115,  0.29267539,  0.10607832,  1.1551053 ,  1.76731577,\n",
    "#         0.31406486,  0.76772426,  0.69169783, -0.83668658,  0.33569889,\n",
    "#         0.64205243, -1.98684006,  0.19627804,  0.63516307, -2.        ,\n",
    "#         0.19437876,  1.83094245,  0.1938955 ,  1.2000416 , -0.27878783,\n",
    "#         1.3122472 ,  0.7280207 , -1.34736464])\n",
    "\n",
    "# Bias results (-2, 2) 1000 iterations \n",
    "\n",
    "# all_weights = np.array([-0.12129605, -1.52853405,  0.19080951, -0.24911903,  1.26105855,\n",
    "#         1.03348761, -1.74095598, -1.16912224,  1.82870315,  1.13246097,\n",
    "#        -1.92250512, -0.29562043,  1.272132  , -0.88395889,  0.41502269,\n",
    "#        -1.62145995, -0.23940828,  0.12258842,  0.5943558 ,  0.96140391,\n",
    "#         1.43295031, -1.69399583,  1.67231008,  1.58961642, -1.99320226,\n",
    "#         1.92954028,  1.13961794,  0.12091686, -1.26790324,  0.71803368,\n",
    "#         0.0305413 , -0.45685277, -1.39528121, -1.61745956, -0.77749755,\n",
    "#        -1.11405011,  1.32369548,  1.48570256, -1.38574018, -1.6245386 ,\n",
    "#        -0.43699971,  2.        , -1.12535292,  1.69962718, -1.24650415,\n",
    "#        -1.20377956,  1.1610347 , -0.61618357,  0.2382405 ,  1.6396031 ,\n",
    "#        -1.99745678,  1.10535453,  1.37851962,  0.74019612,  0.64990188,\n",
    "#        -1.6639427 , -0.08798229,  1.72918302,  0.70704046,  1.31502347,\n",
    "#        -1.10791639, -0.23961386,  0.71266012,  2.        , -0.51581491,\n",
    "#        -0.76351852,  1.5590716 , -0.99454865,  0.53418471,  0.89040827,\n",
    "#         0.74428609, -2.        ,  1.95709056, -1.99993718,  0.97621155,\n",
    "#         1.99979467, -2.        , -1.26537603, -1.10784762, -1.78597337,\n",
    "#        -1.5542085 ,  1.47093082, -0.86410135])\n",
    "\n",
    "# Bias and batch=10 10 000 iterations \n",
    "\n",
    "all_weights = np.array([ 1.30848932,  0.81764324,  1.80799548,  0.34838307, -0.07685203,\n",
    "        1.67280683, -0.86301839, -1.0747434 ,  1.79727309, -0.71171146,\n",
    "       -0.01800825, -1.16365216, -1.95096072,  0.62491266,  0.05381063,\n",
    "        1.71313736, -0.39757288, -0.16410246,  1.90997168,  1.96311439,\n",
    "       -1.3600024 , -1.30905888, -0.93432226,  1.01873819,  0.49787334,\n",
    "       -0.97811568, -1.27805269,  1.76423449,  0.73181911,  0.0686799 ,\n",
    "        1.25301359, -1.34329424,  0.38809433, -1.8002075 , -0.88237253,\n",
    "        0.4462813 , -0.45414581,  1.50860079,  1.64726395,  1.59883219,\n",
    "        1.33293314,  0.74163364,  0.59915971, -0.71005133, -0.1129822 ,\n",
    "        1.35726647, -1.35842979,  0.40672924, -0.250284  ,  0.30929123,\n",
    "       -1.46753851,  0.17188682,  0.81539141,  0.18308485, -1.29143111,\n",
    "        1.35951747,  0.43201388, -0.9777995 , -1.04724225, -1.3436225 ,\n",
    "       -0.13290859,  1.71373924, -1.66564308,  0.91921875,  0.06461448,\n",
    "        0.43124176,  0.33629514, -1.24159101,  1.60238144, -0.04673199,\n",
    "        0.46919664,  0.60200841,  0.9215234 , -1.12965767,  0.57210215,\n",
    "        0.62441395, -0.65487776,  1.99847449, -0.09075696,  0.17961145,\n",
    "        0.76963415, -0.41160485,  0.16902859])\n",
    "\n",
    "# theta1 = all_weights[:40]\n",
    "# theta2 = all_weights[40:]\n",
    "# theta1 = theta1.reshape((10, 4))\n",
    "# theta2 = theta2.reshape((3, 10))\n",
    "\n",
    "theta1 = all_weights[:40]\n",
    "bias1 = all_weights[40:50]\n",
    "theta2 = all_weights[50:80]\n",
    "bias2 = all_weights[80:]\n",
    "theta1 = theta1.reshape((10, 4))\n",
    "theta2 = theta2.reshape((3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.set_weights([theta1.T, bias1, theta2.T, bias2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1543 - acc: 0.9700\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 0s 284us/step - loss: 0.1568 - acc: 0.9700\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 0s 236us/step - loss: 0.1554 - acc: 0.9700\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 0s 369us/step - loss: 0.1540 - acc: 0.9700\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 0s 245us/step - loss: 0.1502 - acc: 0.9700\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 0s 273us/step - loss: 0.1537 - acc: 0.9600\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1493 - acc: 0.9700\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 0s 360us/step - loss: 0.1480 - acc: 0.9700\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 0s 290us/step - loss: 0.1577 - acc: 0.9400\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 0s 292us/step - loss: 0.1487 - acc: 0.9800\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 0s 344us/step - loss: 0.1478 - acc: 0.9700\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 0s 364us/step - loss: 0.1505 - acc: 0.9700\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 0s 317us/step - loss: 0.1603 - acc: 0.9400\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 0s 399us/step - loss: 0.1529 - acc: 0.9600\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 0s 380us/step - loss: 0.1438 - acc: 0.9800\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 0s 301us/step - loss: 0.1482 - acc: 0.9600\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 0s 297us/step - loss: 0.1547 - acc: 0.9700\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 0s 365us/step - loss: 0.1409 - acc: 0.9700\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 0s 349us/step - loss: 0.1501 - acc: 0.9600\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 0s 338us/step - loss: 0.1424 - acc: 0.9600\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 0s 408us/step - loss: 0.1498 - acc: 0.9700\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 0s 439us/step - loss: 0.1302 - acc: 0.9700\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 0s 257us/step - loss: 0.1418 - acc: 0.9800\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 0s 364us/step - loss: 0.1395 - acc: 0.9600\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 0s 357us/step - loss: 0.1376 - acc: 0.9700\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 0s 372us/step - loss: 0.1323 - acc: 0.9700\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 0s 333us/step - loss: 0.1413 - acc: 0.9400\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 0s 376us/step - loss: 0.1387 - acc: 0.9800\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 0s 313us/step - loss: 0.1373 - acc: 0.9800\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 0s 298us/step - loss: 0.1421 - acc: 0.9700\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 0s 363us/step - loss: 0.1340 - acc: 0.9700\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 0s 345us/step - loss: 0.1561 - acc: 0.9400\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 0s 337us/step - loss: 0.1443 - acc: 0.9600\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 0s 419us/step - loss: 0.1306 - acc: 0.9700\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 0s 321us/step - loss: 0.1319 - acc: 0.9700\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 0s 345us/step - loss: 0.1385 - acc: 0.9500\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 0s 384us/step - loss: 0.1318 - acc: 0.9700\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 0s 501us/step - loss: 0.1367 - acc: 0.9600\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 0s 453us/step - loss: 0.1270 - acc: 0.9700\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 0s 391us/step - loss: 0.1312 - acc: 0.9800\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 0s 337us/step - loss: 0.1275 - acc: 0.9700\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 0s 378us/step - loss: 0.1317 - acc: 0.9700\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.1285 - acc: 0.9700\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 0s 371us/step - loss: 0.1366 - acc: 0.9600\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 0s 230us/step - loss: 0.1324 - acc: 0.9600\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 0s 398us/step - loss: 0.1356 - acc: 0.9800\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.1272 - acc: 0.9700\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 0s 331us/step - loss: 0.1290 - acc: 0.9800\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 0s 330us/step - loss: 0.1408 - acc: 0.9600\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 0s 260us/step - loss: 0.1409 - acc: 0.9300\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 0s 340us/step - loss: 0.1256 - acc: 0.9800\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 0s 396us/step - loss: 0.1275 - acc: 0.9700\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 0s 476us/step - loss: 0.1352 - acc: 0.9500\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 0s 266us/step - loss: 0.1311 - acc: 0.9700\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 0s 406us/step - loss: 0.1349 - acc: 0.9500\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 0s 407us/step - loss: 0.1234 - acc: 0.9600\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 0s 255us/step - loss: 0.1333 - acc: 0.9600\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 0s 291us/step - loss: 0.1218 - acc: 0.9600\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 0s 335us/step - loss: 0.1218 - acc: 0.9500\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 0s 244us/step - loss: 0.1216 - acc: 0.9700\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 0s 235us/step - loss: 0.1301 - acc: 0.9600\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 0s 242us/step - loss: 0.1235 - acc: 0.9600\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 0s 286us/step - loss: 0.1230 - acc: 0.9700\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 0s 329us/step - loss: 0.1286 - acc: 0.9500\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 0s 245us/step - loss: 0.1193 - acc: 0.9700\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 0s 254us/step - loss: 0.1157 - acc: 0.9800\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 0s 244us/step - loss: 0.1155 - acc: 0.9600\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 0s 372us/step - loss: 0.1240 - acc: 0.9700\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 0s 304us/step - loss: 0.1210 - acc: 0.9800\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 0s 333us/step - loss: 0.1339 - acc: 0.9400\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 0s 313us/step - loss: 0.1204 - acc: 0.9700\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 0s 314us/step - loss: 0.1140 - acc: 0.9600\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 0s 717us/step - loss: 0.1304 - acc: 0.9600\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 0s 573us/step - loss: 0.1246 - acc: 0.9800\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 0s 574us/step - loss: 0.1226 - acc: 0.9600\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 0s 447us/step - loss: 0.1204 - acc: 0.9700\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 0s 560us/step - loss: 0.1320 - acc: 0.9600\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 0s 317us/step - loss: 0.1213 - acc: 0.9700\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 0s 242us/step - loss: 0.1194 - acc: 0.9700\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 0s 246us/step - loss: 0.1078 - acc: 0.9600\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 0s 278us/step - loss: 0.1382 - acc: 0.9500\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 0s 300us/step - loss: 0.1174 - acc: 0.9600\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 261us/step - loss: 0.1169 - acc: 0.9700\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 0s 242us/step - loss: 0.1191 - acc: 0.9700\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 0s 293us/step - loss: 0.1209 - acc: 0.9600\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 0s 353us/step - loss: 0.1192 - acc: 0.9600\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 0s 261us/step - loss: 0.1241 - acc: 0.9600\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 0s 340us/step - loss: 0.1193 - acc: 0.9600\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 0s 265us/step - loss: 0.1196 - acc: 0.9700\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 0s 336us/step - loss: 0.1201 - acc: 0.9700\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 0s 323us/step - loss: 0.1205 - acc: 0.9600\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 0s 249us/step - loss: 0.1147 - acc: 0.9800\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 0s 303us/step - loss: 0.1121 - acc: 0.9700\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 0s 222us/step - loss: 0.1098 - acc: 0.9800\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 0s 332us/step - loss: 0.1346 - acc: 0.9400\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 0s 277us/step - loss: 0.1123 - acc: 0.9700\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 0s 328us/step - loss: 0.1163 - acc: 0.9600\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 0s 329us/step - loss: 0.1208 - acc: 0.9700\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 0s 242us/step - loss: 0.1186 - acc: 0.9600\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 0s 308us/step - loss: 0.1192 - acc: 0.9600\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 0s 241us/step - loss: 0.1219 - acc: 0.9600\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 0s 305us/step - loss: 0.1176 - acc: 0.9700\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 0s 231us/step - loss: 0.1108 - acc: 0.9800\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 0s 292us/step - loss: 0.1178 - acc: 0.9800\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 0s 364us/step - loss: 0.1119 - acc: 0.9600\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 0s 324us/step - loss: 0.1129 - acc: 0.9800\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 0s 327us/step - loss: 0.1131 - acc: 0.9600\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 0s 293us/step - loss: 0.1249 - acc: 0.9700\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 0s 369us/step - loss: 0.1168 - acc: 0.9500\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 0s 322us/step - loss: 0.1132 - acc: 0.9600\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 0s 318us/step - loss: 0.1125 - acc: 0.9700\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 0s 260us/step - loss: 0.1097 - acc: 0.9800\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 0s 246us/step - loss: 0.1078 - acc: 0.9700\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 0s 222us/step - loss: 0.1247 - acc: 0.9700\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 0s 365us/step - loss: 0.1075 - acc: 0.9900\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 0s 279us/step - loss: 0.1169 - acc: 0.9700\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 0s 337us/step - loss: 0.1054 - acc: 0.9700\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 0s 225us/step - loss: 0.1228 - acc: 0.9700\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 0s 294us/step - loss: 0.1262 - acc: 0.9400\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 0s 335us/step - loss: 0.1309 - acc: 0.9600\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 0s 371us/step - loss: 0.1163 - acc: 0.9700\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 0s 291us/step - loss: 0.1064 - acc: 0.9800\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 0s 319us/step - loss: 0.1062 - acc: 0.9800\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 0s 268us/step - loss: 0.1218 - acc: 0.9800\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 0s 311us/step - loss: 0.0998 - acc: 0.9700\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 0s 340us/step - loss: 0.1164 - acc: 0.9600\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 0s 249us/step - loss: 0.1180 - acc: 0.9600\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 0s 289us/step - loss: 0.1096 - acc: 0.9800\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 0s 211us/step - loss: 0.1108 - acc: 0.9700\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 0s 312us/step - loss: 0.1155 - acc: 0.9600\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 0s 250us/step - loss: 0.1099 - acc: 0.9700\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 0s 269us/step - loss: 0.1140 - acc: 0.9600\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 0s 215us/step - loss: 0.1149 - acc: 0.9600\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 0s 292us/step - loss: 0.1110 - acc: 0.9700\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 0s 371us/step - loss: 0.1251 - acc: 0.9300\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 0s 267us/step - loss: 0.1077 - acc: 0.9800\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 0s 293us/step - loss: 0.1232 - acc: 0.9600\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 0s 235us/step - loss: 0.1037 - acc: 0.9700\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.1064 - acc: 0.9700\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 0s 330us/step - loss: 0.1029 - acc: 0.9600\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 0s 415us/step - loss: 0.1056 - acc: 0.9700\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 0s 254us/step - loss: 0.1096 - acc: 0.9600\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 0s 350us/step - loss: 0.1036 - acc: 0.9700\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 0s 260us/step - loss: 0.1111 - acc: 0.9500\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 0s 294us/step - loss: 0.1047 - acc: 0.9700\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 0s 327us/step - loss: 0.1022 - acc: 0.9700\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 0s 259us/step - loss: 0.1067 - acc: 0.9700\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 0s 295us/step - loss: 0.1093 - acc: 0.9800\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 0s 389us/step - loss: 0.1098 - acc: 0.9800\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 0s 304us/step - loss: 0.1132 - acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1813327b90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 142us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 98.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.34999982,  0.19364983, -0.61618567,  0.5884401 ,  0.8258041 ,\n",
       "         -0.7384186 , -0.37514338,  0.47554633, -0.48200852, -0.1906842 ],\n",
       "        [ 0.6313953 , -0.29363513,  0.27914652,  0.18415543,  0.8910328 ,\n",
       "         -0.6097664 ,  0.60845816,  0.13048267,  0.16767907, -0.07367782],\n",
       "        [-0.82710904, -0.11580761,  0.40092298,  0.54433686, -1.1654577 ,\n",
       "          1.1271871 , -0.52587855, -0.6277064 ,  0.3284272 ,  0.40771842],\n",
       "        [ 0.3242122 , -0.1375943 ,  0.2135342 ,  0.50441736, -0.72839725,\n",
       "          1.5303509 ,  0.05286014, -0.54369485, -0.09211999, -0.6523648 ]],\n",
       "       dtype=float32),\n",
       " array([ 0.07377704, -0.00473215, -0.00650457,  0.0130051 ,  0.31172878,\n",
       "        -0.47183666,  0.        ,  0.0549375 ,  0.        , -0.01525889],\n",
       "       dtype=float32),\n",
       " array([[ 0.79172945,  0.02016998, -0.24258855],\n",
       "        [-0.29962906, -0.1715637 ,  0.5703985 ],\n",
       "        [ 0.4474163 ,  0.33320424, -0.33405593],\n",
       "        [-0.7797515 ,  0.3803962 ,  0.26007226],\n",
       "        [ 1.0807115 , -0.07122085, -1.4979308 ],\n",
       "        [ 0.07418168, -1.166495  ,  1.5835828 ],\n",
       "        [-0.4287253 , -0.59361726, -0.34948072],\n",
       "        [ 0.6111864 , -0.773901  ,  0.24702077],\n",
       "        [-0.3105248 ,  0.4375702 , -0.5783126 ],\n",
       "        [-0.1690552 , -0.5762356 , -0.6521799 ]], dtype=float32),\n",
       " array([-0.01321292,  0.3702023 , -0.356989  ], dtype=float32)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
