{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width            class\n",
       "0           6.9          3.1           5.4          2.1   Iris-virginica\n",
       "1           5.4          3.7           1.5          0.2      Iris-setosa\n",
       "2           5.8          2.7           4.1          1.0  Iris-versicolor\n",
       "3           6.7          3.1           4.4          1.4  Iris-versicolor\n",
       "4           5.1          3.8           1.6          0.2      Iris-setosa\n",
       "5           5.1          2.5           3.0          1.1  Iris-versicolor\n",
       "6           5.5          4.2           1.4          0.2      Iris-setosa\n",
       "7           6.4          2.8           5.6          2.2   Iris-virginica\n",
       "8           6.3          2.8           5.1          1.5   Iris-virginica\n",
       "9           4.9          3.1           1.5          0.1      Iris-setosa"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('iris.data', header=0)\n",
    "\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.121097</td>\n",
       "      <td>-0.012273</td>\n",
       "      <td>0.252367</td>\n",
       "      <td>0.360533</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.005485</td>\n",
       "      <td>-0.080455</td>\n",
       "      <td>0.194396</td>\n",
       "      <td>0.280533</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.030802</td>\n",
       "      <td>-0.125909</td>\n",
       "      <td>0.020483</td>\n",
       "      <td>-0.039467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.019831</td>\n",
       "      <td>-0.035000</td>\n",
       "      <td>0.107440</td>\n",
       "      <td>0.120533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.106751</td>\n",
       "      <td>0.078636</td>\n",
       "      <td>-0.327343</td>\n",
       "      <td>-0.399467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.106751</td>\n",
       "      <td>-0.171364</td>\n",
       "      <td>-0.066473</td>\n",
       "      <td>-0.079467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.146414</td>\n",
       "      <td>0.033182</td>\n",
       "      <td>0.136425</td>\n",
       "      <td>0.080533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.032489</td>\n",
       "      <td>-0.057727</td>\n",
       "      <td>0.136425</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.043460</td>\n",
       "      <td>-0.148636</td>\n",
       "      <td>-0.008502</td>\n",
       "      <td>-0.079467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.057806</td>\n",
       "      <td>0.055909</td>\n",
       "      <td>0.324831</td>\n",
       "      <td>0.520533</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width  class\n",
       "126      0.121097    -0.012273      0.252367     0.360533      2\n",
       "14      -0.005485    -0.080455      0.194396     0.280533      2\n",
       "78      -0.030802    -0.125909      0.020483    -0.039467      1\n",
       "106      0.019831    -0.035000      0.107440     0.120533      1\n",
       "86      -0.106751     0.078636     -0.327343    -0.399467      0\n",
       "141     -0.106751    -0.171364     -0.066473    -0.079467      1\n",
       "52       0.146414     0.033182      0.136425     0.080533      1\n",
       "34       0.032489    -0.057727      0.136425     0.000533      1\n",
       "107     -0.043460    -0.148636     -0.008502    -0.079467      1\n",
       "58       0.057806     0.055909      0.324831     0.520533      2"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data standartization\n",
    "for label in list(dataset)[:-1]:\n",
    "    dataset[label] = (dataset[label] - dataset[label].mean()) / dataset[label].abs().max()\n",
    "\n",
    "# Mapping classes to numbers.\n",
    "dataset['class'] = dataset['class'].map({'Iris-setosa':0, \n",
    "                                         'Iris-versicolor':1, \n",
    "                                         'Iris-virginica':2}).astype(int)\n",
    "\n",
    "train = dataset[:100]\n",
    "test = dataset[100:]\n",
    "\n",
    "X = train.as_matrix(['sepal length', \n",
    "                   'sepal width', \n",
    "                   'petal length', \n",
    "                   'petal width'])\n",
    "y = train.as_matrix(['class'])\n",
    "y = to_categorical(y)\n",
    "\n",
    "X_test = test.as_matrix(['sepal length', \n",
    "                   'sepal width', \n",
    "                   'petal length', \n",
    "                   'petal width'])\n",
    "y_test = test.as_matrix(['class'])\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing basic weights.\n",
    "\n",
    "all_weights = np.array([ 0.94687211, -1.17460209,  1.31392388,  1.80457367,  0.1673272 ,\n",
    "       -0.61202518, -1.43140639,  1.09407974, -1.32407377, -0.22533852,\n",
    "        0.85725644, -0.65684114,  0.35252209, -0.54304552, -1.5368403 ,\n",
    "        0.31183973, -0.88354785, -0.37191471,  1.05979869,  0.65025884,\n",
    "       -0.33369859, -1.65704745,  1.24157308,  0.54328191,  0.61338311,\n",
    "       -0.38718278,  0.38212946, -0.57253222,  0.59588837,  1.94174821,\n",
    "        0.04292019,  1.95610101, -0.81583466,  1.24463668, -0.41745033,\n",
    "        1.76090925,  1.77253165,  1.96020692, -0.11760402,  1.99500498,\n",
    "        1.90604524, -0.13210263, -0.88899273, -0.9338501 ,  1.58071441,\n",
    "        1.92337724, -0.04912468,  1.68260161,  1.50712985,  1.54080191,\n",
    "       -0.99211549, -0.68746827,  0.34921015,  0.39645783, -1.00724935,\n",
    "       -1.61396399,  0.12846123, -1.15706667, -0.67966803, -0.11938515,\n",
    "       -0.41262891,  0.10756836,  1.74959194, -0.74501946, -0.63139459,\n",
    "       -1.1364828 , -0.88444955, -0.59889404, -0.99339974, -1.76182285,\n",
    "        1.90056841,  1.04310259, -0.03934248, -1.98156093,  1.75311117,\n",
    "        1.41984593, -0.55315984,  1.85893195,  1.85426186,  1.81138514,\n",
    "        1.05757455,  0.72267061,  0.35654629])\n",
    "\n",
    "#  Reshaping 1D input weights to 2 2D matrix and bias vectors.\n",
    "theta1 = all_weights[:40]\n",
    "bias1 = all_weights[40:50]\n",
    "theta2 = all_weights[50:80]\n",
    "bias2 = all_weights[80:]\n",
    "theta1 = theta1.reshape((10, 4))\n",
    "theta2 = theta2.reshape((3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 4ms/step\n",
      "50/50 [==============================] - 0s 162us/step\n"
     ]
    }
   ],
   "source": [
    "model.set_weights([theta1.T, bias1, theta2.T, bias2])\n",
    "\n",
    "scores_train = model.evaluate(X, y)\n",
    "scores_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "acc: 31.00%\n",
      "Test:\n",
      "acc: 38.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\\n%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "print(\"Test:\\n%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 10.4502 - acc: 0.3100\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 0s 329us/step - loss: 9.7492 - acc: 0.3100\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 0s 368us/step - loss: 9.0767 - acc: 0.3100\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 0s 531us/step - loss: 8.4509 - acc: 0.3100\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 0s 398us/step - loss: 7.8540 - acc: 0.3100\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 0s 488us/step - loss: 7.2790 - acc: 0.3100\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 0s 379us/step - loss: 6.6997 - acc: 0.3100\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 0s 393us/step - loss: 6.2139 - acc: 0.3100\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 0s 407us/step - loss: 5.7248 - acc: 0.3500\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 0s 381us/step - loss: 5.2681 - acc: 0.5200\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 0s 454us/step - loss: 4.7590 - acc: 0.5900\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 0s 321us/step - loss: 4.1676 - acc: 0.6300\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 0s 514us/step - loss: 3.5742 - acc: 0.6400\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 0s 440us/step - loss: 3.0464 - acc: 0.6400\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 0s 467us/step - loss: 2.5866 - acc: 0.6400\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 0s 609us/step - loss: 2.1947 - acc: 0.6400\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 0s 452us/step - loss: 1.8584 - acc: 0.6400\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 0s 458us/step - loss: 1.5741 - acc: 0.6400\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 0s 396us/step - loss: 1.3427 - acc: 0.6400\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 0s 499us/step - loss: 1.1632 - acc: 0.6400\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 0s 409us/step - loss: 1.0175 - acc: 0.6600\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 0s 382us/step - loss: 0.8995 - acc: 0.6700\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 0s 425us/step - loss: 0.8082 - acc: 0.6900\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 0s 415us/step - loss: 0.7373 - acc: 0.7500\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 0s 399us/step - loss: 0.6825 - acc: 0.7700\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 0s 384us/step - loss: 0.6397 - acc: 0.8000\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 0s 371us/step - loss: 0.6054 - acc: 0.8300\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 0s 335us/step - loss: 0.5788 - acc: 0.8400\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 0s 336us/step - loss: 0.5568 - acc: 0.8500\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 0s 338us/step - loss: 0.5390 - acc: 0.8800\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 0s 345us/step - loss: 0.5241 - acc: 0.8800\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 0s 366us/step - loss: 0.5127 - acc: 0.9000\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 0s 470us/step - loss: 0.5020 - acc: 0.9000\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 0s 503us/step - loss: 0.4930 - acc: 0.9100\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 0s 385us/step - loss: 0.4855 - acc: 0.9400\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 0s 337us/step - loss: 0.4797 - acc: 0.9400\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.4734 - acc: 0.9400\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 0s 418us/step - loss: 0.4682 - acc: 0.9400\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 0s 410us/step - loss: 0.4635 - acc: 0.9400\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 0s 503us/step - loss: 0.4588 - acc: 0.9400\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 0s 604us/step - loss: 0.4550 - acc: 0.9400\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 0s 746us/step - loss: 0.4512 - acc: 0.9400\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 0s 620us/step - loss: 0.4474 - acc: 0.9400\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 0s 332us/step - loss: 0.4439 - acc: 0.9500\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 0s 423us/step - loss: 0.4407 - acc: 0.9400\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 0s 378us/step - loss: 0.4370 - acc: 0.9500\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 0s 331us/step - loss: 0.4335 - acc: 0.9400\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 0s 327us/step - loss: 0.4305 - acc: 0.9400\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 0s 399us/step - loss: 0.4272 - acc: 0.9400\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 0s 377us/step - loss: 0.4244 - acc: 0.9400\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 0s 340us/step - loss: 0.4215 - acc: 0.9600\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 0s 352us/step - loss: 0.4182 - acc: 0.9500\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 0s 549us/step - loss: 0.4158 - acc: 0.9600\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 0s 690us/step - loss: 0.4130 - acc: 0.9600\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 0s 554us/step - loss: 0.4100 - acc: 0.9600\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 0s 324us/step - loss: 0.4073 - acc: 0.9600\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 0s 325us/step - loss: 0.4047 - acc: 0.9600\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 0s 342us/step - loss: 0.4019 - acc: 0.9600\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 0s 329us/step - loss: 0.3995 - acc: 0.9600\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 0s 403us/step - loss: 0.3969 - acc: 0.9600\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 0s 370us/step - loss: 0.3943 - acc: 0.9600\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 0s 356us/step - loss: 0.3918 - acc: 0.9600\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 0s 355us/step - loss: 0.3892 - acc: 0.9600\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 0s 421us/step - loss: 0.3868 - acc: 0.9600\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 0s 426us/step - loss: 0.3845 - acc: 0.9600\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 0s 348us/step - loss: 0.3816 - acc: 0.9600\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 0s 341us/step - loss: 0.3797 - acc: 0.9600\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 0s 327us/step - loss: 0.3774 - acc: 0.9600\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 0s 324us/step - loss: 0.3745 - acc: 0.9600\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 0s 278us/step - loss: 0.3724 - acc: 0.9600\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 0s 322us/step - loss: 0.3699 - acc: 0.9600\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 0s 393us/step - loss: 0.3676 - acc: 0.9600\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 0s 345us/step - loss: 0.3653 - acc: 0.9600\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.3633 - acc: 0.9600\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 0s 367us/step - loss: 0.3612 - acc: 0.9600\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 0s 494us/step - loss: 0.3586 - acc: 0.9600\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 0s 486us/step - loss: 0.3565 - acc: 0.9600\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 0s 352us/step - loss: 0.3542 - acc: 0.9600\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 0s 324us/step - loss: 0.3519 - acc: 0.9600\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 0s 392us/step - loss: 0.3498 - acc: 0.9600\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 0s 382us/step - loss: 0.3482 - acc: 0.9600\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 0s 342us/step - loss: 0.3455 - acc: 0.9600\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 349us/step - loss: 0.3435 - acc: 0.9600\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 0s 457us/step - loss: 0.3416 - acc: 0.9600\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 0s 327us/step - loss: 0.3397 - acc: 0.9600\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 0s 329us/step - loss: 0.3376 - acc: 0.9600\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 0s 410us/step - loss: 0.3356 - acc: 0.9600\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 0s 406us/step - loss: 0.3342 - acc: 0.9600\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 0s 405us/step - loss: 0.3324 - acc: 0.9600\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 0s 403us/step - loss: 0.3302 - acc: 0.9600\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 0s 420us/step - loss: 0.3283 - acc: 0.9600\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 0s 380us/step - loss: 0.3268 - acc: 0.9600\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 0s 261us/step - loss: 0.3248 - acc: 0.9600\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 0s 361us/step - loss: 0.3229 - acc: 0.9600\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 0s 296us/step - loss: 0.3211 - acc: 0.9600\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 0s 357us/step - loss: 0.3201 - acc: 0.9600\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 0s 256us/step - loss: 0.3177 - acc: 0.9600\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 0s 336us/step - loss: 0.3162 - acc: 0.9600\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 0s 309us/step - loss: 0.3145 - acc: 0.9600\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 0s 266us/step - loss: 0.3134 - acc: 0.9600\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 0s 305us/step - loss: 0.3114 - acc: 0.9600\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 0s 343us/step - loss: 0.3095 - acc: 0.9600\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 0s 267us/step - loss: 0.3080 - acc: 0.9600\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 0s 337us/step - loss: 0.3064 - acc: 0.9600\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 0s 243us/step - loss: 0.3049 - acc: 0.9600\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 0s 398us/step - loss: 0.3030 - acc: 0.9600\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 0s 389us/step - loss: 0.3015 - acc: 0.9600\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 0s 252us/step - loss: 0.3001 - acc: 0.9600\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 0s 288us/step - loss: 0.2984 - acc: 0.9600\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 0s 346us/step - loss: 0.2970 - acc: 0.9600\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 0s 296us/step - loss: 0.2951 - acc: 0.9600\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 0s 358us/step - loss: 0.2934 - acc: 0.9600\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 0s 283us/step - loss: 0.2920 - acc: 0.9600\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 0s 363us/step - loss: 0.2906 - acc: 0.9600\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 0s 285us/step - loss: 0.2888 - acc: 0.9500\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 0s 276us/step - loss: 0.2872 - acc: 0.9600\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 0s 267us/step - loss: 0.2857 - acc: 0.9600\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 0s 274us/step - loss: 0.2843 - acc: 0.9600\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 0s 357us/step - loss: 0.2836 - acc: 0.9600\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 0s 271us/step - loss: 0.2817 - acc: 0.9700\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 0s 343us/step - loss: 0.2803 - acc: 0.9600\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 0s 395us/step - loss: 0.2788 - acc: 0.9500\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 0s 268us/step - loss: 0.2777 - acc: 0.9600\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 0s 245us/step - loss: 0.2762 - acc: 0.9700\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 0s 355us/step - loss: 0.2750 - acc: 0.9700\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 0s 240us/step - loss: 0.2737 - acc: 0.9700\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 0s 272us/step - loss: 0.2727 - acc: 0.9600\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 0s 364us/step - loss: 0.2713 - acc: 0.9700\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 0s 400us/step - loss: 0.2699 - acc: 0.9600\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 0s 252us/step - loss: 0.2686 - acc: 0.9500\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 0s 391us/step - loss: 0.2681 - acc: 0.9500\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 0s 281us/step - loss: 0.2663 - acc: 0.9700\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 0s 355us/step - loss: 0.2651 - acc: 0.9600\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 0s 273us/step - loss: 0.2642 - acc: 0.9600\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 0s 457us/step - loss: 0.2632 - acc: 0.9500\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 0s 400us/step - loss: 0.2621 - acc: 0.9600\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 0s 274us/step - loss: 0.2609 - acc: 0.9500\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 0s 355us/step - loss: 0.2599 - acc: 0.9500\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 0s 277us/step - loss: 0.2585 - acc: 0.9600\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 0s 306us/step - loss: 0.2582 - acc: 0.9700\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 0s 354us/step - loss: 0.2566 - acc: 0.9700\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 0s 272us/step - loss: 0.2562 - acc: 0.9600\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 0s 285us/step - loss: 0.2550 - acc: 0.9700\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 0s 351us/step - loss: 0.2544 - acc: 0.9600\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 0s 260us/step - loss: 0.2529 - acc: 0.9700\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 0s 361us/step - loss: 0.2525 - acc: 0.9500\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 0s 264us/step - loss: 0.2510 - acc: 0.9700\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 0s 414us/step - loss: 0.2507 - acc: 0.9600\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 0s 291us/step - loss: 0.2496 - acc: 0.9700\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 0s 343us/step - loss: 0.2491 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1821e6b310>"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 133us/step\n",
      "50/50 [==============================] - 0s 147us/step\n"
     ]
    }
   ],
   "source": [
    "scores_train = model.evaluate(X, y)\n",
    "scores_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "acc: 97.00%\n",
      "Test:\n",
      "acc: 96.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\\n%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "print(\"Test:\\n%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.12129605,  1.1414499 ,  1.8287032 ,  1.197967  , -0.23940828,\n",
       "          1.5388327 , -2.0305076 , -1.2679032 , -1.3921349 ,  1.2141073 ],\n",
       "        [-1.528534  ,  1.2004582 ,  1.132461  , -0.9021921 ,  0.12258842,\n",
       "         -1.7715137 ,  1.9272462 ,  0.7180337 , -1.5698389 ,  1.557972  ],\n",
       "        [ 0.1908095 , -2.1938727 , -1.9225051 ,  0.18214469,  0.5943558 ,\n",
       "          2.061206  ,  1.0078062 ,  0.0305413 , -0.8459417 , -1.7095819 ],\n",
       "        [-0.24911903, -1.6868356 , -0.29562044, -1.9896674 ,  0.9614039 ,\n",
       "          2.1186361 , -0.11543606, -0.45685276, -1.1881891 , -2.0133252 ]],\n",
       "       dtype=float32),\n",
       " array([-0.4369997 ,  0.83060896, -1.125353  ,  0.6968958 , -1.2465042 ,\n",
       "         0.4252797 , -0.06002772, -0.6161836 ,  0.10193942,  0.90474343],\n",
       "       dtype=float32),\n",
       " array([[-1.9974568 , -1.1079164 ,  0.74428606],\n",
       "        [ 1.3465135 , -0.78044224, -1.7003281 ],\n",
       "        [ 1.3785197 ,  0.71266013,  1.9570906 ],\n",
       "        [ 0.7566066 ,  1.8074772 , -1.8238305 ],\n",
       "        [ 0.64990187, -0.5158149 ,  0.97621155],\n",
       "        [-1.9250046 , -0.87008476,  2.367424  ],\n",
       "        [-0.05205862,  1.3140781 , -1.7909278 ],\n",
       "        [ 1.7291831 , -0.9945487 , -1.265376  ],\n",
       "        [ 0.87581617,  0.35152644, -1.0939636 ],\n",
       "        [ 1.7039464 ,  0.46797618, -1.7524608 ]], dtype=float32),\n",
       " array([-1.8978047,  1.3257744, -0.3753495], dtype=float32)]"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
